import tensorflow as tf
from random import randint
import numpy as np
import matplotlib.pyplot as pyplot
import keras

from generator_model import generator
from discriminator_model import discriminator
from dataset_creation import dataset

EPOCHS = 100
BATCH_SIZE = 8

gpus = tf.config.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(gpus[0], True)
except Exception as e:
    print(e)

for layer in discriminator.layers:
    layer.trainable = False

output = discriminator(generator.output)

composite_model = tf.keras.Model(generator.input, output)

composite_model.compile(
    loss = ['binary_crossentropy', 'sparse_categorical_crossentropy'],
    optimizer = keras.optimizers.Adam(lr = 0.0001, beta_1=0.5),
)

# from https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/
def generate_latent_points(latent_dim, samples, classes=10):
    # generate points in the latent space
    x_input = np.random.randn(latent_dim * samples)
    # reshape into a batch of inputs for the network
    z_input = x_input.reshape(samples, latent_dim)
    # generate labels
    labels = randint(0, classes, samples)
    return [z_input, labels]

# from https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/
def generate_real_samples(dataset, samples):
    images, labels = dataset
    i = randint(0, images.shape[0], samples)
    # select images and labels
    X, labels = images[i], labels[i]
    # generate class labels
    y = np.ones((samples, 1))
    return (X, labels), y

# from https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/
def generate_fake_samples(generator, latent_dim, samples):
    # generate points in latent space
    z_input, labels_input = generate_latent_points(latent_dim, samples)
    # predict outputs
    images = generator.predict([z_input, labels_input])
    # create class labels
    y = np.zeros((samples, 1))
    return (images, labels_input), y

# from https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/
# generate samples and save as a plot and save the model
def save_models(step, g_model, d_model, latent_dim, n_samples=100):
 # prepare fake examples
    [X, _], _ = generate_fake_samples(g_model, latent_dim, n_samples)
    # scale from [-1,1] to [0,1]
    X = (X + 1) / 2.0
    # plot images
    for i in range(100):
        # define subplot
        pyplot.subplot(10, 10, 1 + i)
        # turn off axis
        pyplot.axis('off')
        # plot raw pixel data
        pyplot.imshow(X[i, :, :, 0])
    # save plot to file
    filename1 = 'generated_plot_%04d.png' % (step+1)
    pyplot.savefig(filename1)
    pyplot.close()
    # save the generator model
    filename2 = 'g_model.h5' % (step+1)
    filename3 = 'd_model_%04d.json' % (step+1)
    g_model.save(filename2)
    d_model.save(filename3)
    print('>Saved: %s and %s' % (filename1, filename2))

def train(g_model, d_model, gan_model, dataset, latent_dim):
    # calculate the number of batches per training epoch
    bat_per_epo = int(dataset[0].shape[0] / BATCH_SIZE)
    # calculate the number of training iterations
    n_steps = bat_per_epo * EPOCHS
    # calculate the size of half a batch of samples
    half_batch = int(BATCH_SIZE / 2)
    # manually enumerate epochs
    for i in range(n_steps):
        # get randomly selected 'real' samples
        [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)
        # update discriminator model weights
        _,d_r1,d_r2 = d_model.train_on_batch(X_real, [y_real, labels_real])
        # generate 'fake' examples
        [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
        # update discriminator model weights
        _,d_f,d_f2 = d_model.train_on_batch(X_fake, [y_fake, labels_fake])
        # prepare points in latent space as input for the generator
        [z_input, z_labels] = generate_latent_points(latent_dim, BATCH_SIZE)
        # create inverted labels for the fake samples
        y_gan = np.ones((BATCH_SIZE, 1))
        # update the generator via the discriminator's error
        _,g_1,g_2 = gan_model.train_on_batch([z_input, z_labels], [y_gan, z_labels])
        # summarize loss on this batch
        print('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % (i+1, d_r1,d_r2, d_f,d_f2, g_1,g_2))
        # evaluate the model performance every 'epoch'
        if (i+1) % (bat_per_epo * 10) == 0:
            save_models(i, g_model, latent_dim)

train(generator, discriminator, composite_model, dataset, 1000)